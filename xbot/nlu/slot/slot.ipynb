{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertModel\n",
    "import numpy as np\n",
    "import random\n",
    "from transformers import BertTokenizer\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import zipfile\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from convlab2.nlu.jointBERT.dataloader import Dataloader\n",
    "from convlab2.nlu.jointBERT.jointBERT import JointBERT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataloader:\n",
    "    def __init__(self, intent_vocab, tag_vocab, pretrained_weights):\n",
    "        self.tag_vocab = tag_vocab\n",
    "        self.intent_dim = len(intent_vocab)\n",
    "        self.tag_dim = len(tag_vocab)\n",
    "        self.id2intent = dict([(i, x) for i, x in enumerate(intent_vocab)])\n",
    "        self.intent2id = dict([(x, i) for i, x in enumerate(intent_vocab)])\n",
    "        self.id2tag = dict([(i, x) for i, x in enumerate(tag_vocab)])\n",
    "        self.tag2id = dict([(x, i) for i, x in enumerate(tag_vocab)])\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(pretrained_weights)\n",
    "        self.data = {}\n",
    "        self.intent_weight = [1] * len(self.intent2id)\n",
    "\n",
    "    def load_data(self, data, data_key, cut_sen_len, use_bert_tokenizer=True):\n",
    "        \"\"\"\n",
    "        sample representation: [list of words, list of tags, list of intents, original dialog act]\n",
    "        :param data_key: train/val/test\n",
    "        :param data:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.data[data_key] = data\n",
    "        max_sen_len, max_context_len = 0, 0\n",
    "        sen_len = []\n",
    "        context_len = []\n",
    "        for d in self.data[data_key]:\n",
    "            max_sen_len = max(max_sen_len, len(d[0]))#计算最大句子长度\n",
    "            sen_len.append(len(d[0]))\n",
    "            if cut_sen_len > 0:\n",
    "                d[0] = d[0][:cut_sen_len]\n",
    "                d[1] = d[1][:cut_sen_len]\n",
    "                d[3] = [' '.join(s.split()[:cut_sen_len]) for s in d[3]]\n",
    "\n",
    "            d[3] = self.tokenizer.encode('[CLS] ' + ' [SEP] '.join(d[3]))\n",
    "            max_context_len = max(max_context_len, len(d[3]))\n",
    "            context_len.append(len(d[3]))\n",
    "\n",
    "            if use_bert_tokenizer:\n",
    "                word_seq, tag_seq, new2ori = self.bert_tokenize(d[0], d[1])\n",
    "            else:\n",
    "                word_seq = d[0]\n",
    "                tag_seq = d[1]\n",
    "                new2ori = None\n",
    "            d.append(new2ori)\n",
    "            d.append(word_seq)\n",
    "            d.append(self.seq_tag2id(tag_seq))\n",
    "\n",
    "\n",
    "\n",
    "    def bert_tokenize(self, word_seq, tag_seq):\n",
    "        split_tokens = []\n",
    "        new_tag_seq = []\n",
    "        new2ori = {}\n",
    "        basic_tokens = self.tokenizer.basic_tokenizer.tokenize(' '.join(word_seq))\n",
    "        accum = ''\n",
    "        i, j = 0, 0\n",
    "        for i, token in enumerate(basic_tokens):\n",
    "            if (accum + token).lower() == word_seq[j].lower():\n",
    "                accum = ''\n",
    "            else:\n",
    "                accum += token\n",
    "            for sub_token in self.tokenizer.wordpiece_tokenizer.tokenize(basic_tokens[i]):\n",
    "                new2ori[len(new_tag_seq)] = j\n",
    "                split_tokens.append(sub_token)\n",
    "                new_tag_seq.append(tag_seq[j])\n",
    "            if accum == '':\n",
    "                j += 1\n",
    "        return split_tokens, new_tag_seq, new2ori\n",
    "\n",
    "    def seq_tag2id(self, tags):\n",
    "        return [self.tag2id[x] for x in tags if x in self.tag2id]\n",
    "\n",
    "    def seq_id2tag(self, ids):\n",
    "        return [self.id2tag[x] for x in ids]\n",
    "\n",
    "    def seq_intent2id(self, intents):\n",
    "        return [self.intent2id[x] for x in intents if x in self.intent2id]\n",
    "\n",
    "    def seq_id2intent(self, ids):\n",
    "        return [self.id2intent[x] for x in ids]\n",
    "\n",
    "    def pad_batch(self, batch_data):\n",
    "        batch_size = len(batch_data)\n",
    "        max_seq_len  = max([len(x[0]) for x in batch_data]) + 2\n",
    "        word_mask_tensor = torch.zeros((batch_size, max_seq_len), dtype=torch.long)\n",
    "        word_seq_tensor = torch.zeros((batch_size, max_seq_len), dtype=torch.long)\n",
    "        tag_mask_tensor = torch.zeros((batch_size, max_seq_len), dtype=torch.long)\n",
    "        tag_seq_tensor = torch.zeros((batch_size, max_seq_len), dtype=torch.long)\n",
    "        context_max_seq_len = max([len(x[3]) for x in batch_data])\n",
    "        context_mask_tensor = torch.zeros((batch_size, context_max_seq_len), dtype=torch.long)\n",
    "        context_seq_tensor = torch.zeros((batch_size, context_max_seq_len), dtype=torch.long)\n",
    "        for i in range(batch_size):\n",
    "            words = batch_data[i][-2]\n",
    "            tags = batch_data[i][-1]\n",
    "            words = ['[CLS]'] + words + ['[SEP]']\n",
    "            indexed_tokens = self.tokenizer.convert_tokens_to_ids(words)\n",
    "            sen_len = len(words)\n",
    "            word_seq_tensor[i, :sen_len] = torch.LongTensor([indexed_tokens])\n",
    "            tag_seq_tensor[i, 1:sen_len-1] = torch.LongTensor(tags)\n",
    "            word_mask_tensor[i, :sen_len] = torch.LongTensor([1] * sen_len)\n",
    "            tag_mask_tensor[i, 1:sen_len-1] = torch.LongTensor([1] * (sen_len-2))\n",
    "            context_len = len(batch_data[i][3])\n",
    "\n",
    "            context_seq_tensor[i, :context_len] = torch.LongTensor([batch_data[i][3]])\n",
    "            context_mask_tensor[i, :context_len] = torch.LongTensor([1] * context_len)\n",
    "\n",
    "        return word_seq_tensor, tag_seq_tensor,  word_mask_tensor, tag_mask_tensor, context_seq_tensor, context_mask_tensor\n",
    "\n",
    "    def get_train_batch(self, batch_size):\n",
    "        batch_data = random.choices(self.data['train'], k=batch_size)\n",
    "        return self.pad_batch(batch_data)\n",
    "\n",
    "    def yield_batches(self, batch_size, data_key):\n",
    "        batch_num = math.ceil(len(self.data[data_key]) / batch_size)\n",
    "        for i in range(batch_num):\n",
    "            batch_data = self.data[data_key][i * batch_size:(i + 1) * batch_size]\n",
    "            yield self.pad_batch(batch_data), batch_data, len(batch_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointBERT(nn.Module):\n",
    "    def __init__(self, model_config, device, slot_dim):\n",
    "        super(JointBERT, self).__init__()\n",
    "        self.slot_num_labels = slot_dim\n",
    "        self.device = device\n",
    "        #print(model_config['pretrained_weights'])\n",
    "        self.bert = BertModel.from_pretrained(model_config['pretrained_weights'])\n",
    "        self.dropout = nn.Dropout(model_config['dropout'])\n",
    "        self.context = model_config['context']\n",
    "        self.finetune = model_config['finetune']\n",
    "        self.context_grad = model_config['context_grad']\n",
    "        self.hidden_units = model_config['hidden_units']\n",
    "        if self.hidden_units > 0:\n",
    "            if self.context:\n",
    "                self.slot_classifier = nn.Linear(self.hidden_units, self.slot_num_labels)\n",
    "                self.slot_hidden = nn.Linear(2 * self.bert.config.hidden_size, self.hidden_units)\n",
    "            else:\n",
    "                self.slot_classifier = nn.Linear(self.hidden_units, self.slot_num_labels)\n",
    "                self.slot_hidden = nn.Linear(self.bert.config.hidden_size, self.hidden_units)\n",
    "            nn.init.xavier_uniform_(self.slot_hidden.weight)\n",
    "        else:\n",
    "            if self.context:\n",
    "                self.slot_classifier = nn.Linear(2 * self.bert.config.hidden_size, self.slot_num_labels)\n",
    "            else:\n",
    "                self.slot_classifier = nn.Linear(self.bert.config.hidden_size, self.slot_num_labels)\n",
    "        nn.init.xavier_uniform_(self.slot_classifier.weight)\n",
    "        self.slot_loss_fct = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, word_seq_tensor, word_mask_tensor, tag_seq_tensor=None, tag_mask_tensor=None,\n",
    "             context_seq_tensor=None, context_mask_tensor=None):\n",
    "        if not self.finetune:\n",
    "            self.bert.eval()\n",
    "            with torch.no_grad():\n",
    "                outputs = self.bert(input_ids=word_seq_tensor,\n",
    "                                    attention_mask=word_mask_tensor)\n",
    "        else:\n",
    "            outputs = self.bert(input_ids=word_seq_tensor,\n",
    "                                attention_mask=word_mask_tensor)\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        if self.context and (context_seq_tensor is not None):\n",
    "            if not self.finetune or not self.context_grad:\n",
    "                with torch.no_grad():\n",
    "                    context_output = self.bert(input_ids=context_seq_tensor, attention_mask=context_mask_tensor)[1]\n",
    "            else:\n",
    "                context_output = self.bert(input_ids=context_seq_tensor, attention_mask=context_mask_tensor)[1]\n",
    "            sequence_output = torch.cat(\n",
    "                [context_output.unsqueeze(1).repeat(1, sequence_output.size(1), 1),\n",
    "                 sequence_output], dim=-1)\n",
    "            pooled_output = torch.cat([context_output, pooled_output], dim=-1)\n",
    "\n",
    "        if self.hidden_units > 0:\n",
    "            sequence_output = nn.functional.relu(self.slot_hidden(self.dropout(sequence_output)))\n",
    "            # pooled_output = nn.functional.relu(self.intent_hidden(self.dropout(pooled_output)))\n",
    "\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        slot_logits = self.slot_classifier(sequence_output)\n",
    "        outputs = (slot_logits,)\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        outputs= outputs\n",
    "        if tag_seq_tensor is not None:\n",
    "            active_tag_loss = tag_mask_tensor.view(-1) == 1\n",
    "            active_tag_logits = slot_logits.view(-1, self.slot_num_labels)[active_tag_loss]\n",
    "            active_tag_labels = tag_seq_tensor.view(-1)[active_tag_loss]\n",
    "            slot_loss = self.slot_loss_fct(active_tag_logits, active_tag_labels)\n",
    "            outputs = outputs + (slot_loss,)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "config_path = './crosswoz_all_context.json'\n",
    "config = json.load(open(config_path))\n",
    "data_dir = config['data_dir']\n",
    "output_dir = config['output_dir']\n",
    "log_dir = config['log_dir']\n",
    "DEVICE = config['DEVICE']\n",
    "\n",
    "set_seed(config['seed'])\n",
    "\n",
    "if 'crosswoz' in data_dir:\n",
    "    print('-' * 20 + 'dataset:crosswoz' + '-' * 20)\n",
    "    from convlab2.nlu.jointBERT.crosswoz.postprocess import is_slot_da, calculateF1, recover_intent\n",
    "\n",
    "intent_vocab = json.load(open(os.path.join(data_dir, 'intent_vocab.json'),encoding=\"utf-8\"))\n",
    "tag_vocab = json.load(open(os.path.join(data_dir, 'tag_vocab.json'),encoding=\"utf-8\"))\n",
    "dataloader = Dataloader(intent_vocab=intent_vocab, tag_vocab=tag_vocab,\n",
    "                        pretrained_weights=config['model']['pretrained_weights'])\n",
    "print('tag num:', len(tag_vocab))\n",
    "for data_key in ['train', 'val', 'test']:\n",
    "    dataloader.load_data(json.load(open(os.path.join(data_dir, '{}_data.json'.format(data_key)),encoding=\"utf-8\")), data_key,\n",
    "                         cut_sen_len=config['cut_sen_len'], use_bert_tokenizer=config['use_bert_tokenizer'])\n",
    "    print('{} set size: {}'.format(data_key, len(dataloader.data[data_key])))\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "\n",
    "model = JointBERT(config['model'], DEVICE, dataloader.tag_dim )\n",
    "model.to(DEVICE)\n",
    "\n",
    "if config['model']['finetune']:\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if\n",
    "                    not any(nd in n for nd in no_decay) and p.requires_grad],\n",
    "         'weight_decay': config['model']['weight_decay']},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay) and p.requires_grad],\n",
    "         'weight_decay': 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=config['model']['learning_rate'],\n",
    "                      eps=config['model']['adam_epsilon'])\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=config['model']['warmup_steps'],\n",
    "                                                num_training_steps=config['model']['max_step'])\n",
    "else:\n",
    "    for n, p in model.named_parameters():\n",
    "        if 'bert' in n:\n",
    "            p.requires_grad = False\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                                 lr=config['model']['learning_rate'])\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape, param.device, param.requires_grad)\n",
    "\n",
    "max_step = config['model']['max_step']\n",
    "check_step = config['model']['check_step']\n",
    "batch_size = config['model']['batch_size']\n",
    "model.zero_grad()\n",
    "train_slot_loss=0\n",
    "best_val_f1 = 0.\n",
    "\n",
    "writer.add_text('config', json.dumps(config))\n",
    "\n",
    "for step in range(1, max_step + 1):\n",
    "    model.train()\n",
    "    batched_data = dataloader.get_train_batch(batch_size)\n",
    "\n",
    "    batched_data = tuple(t.to(DEVICE) for t in batched_data)\n",
    "    word_seq_tensor, tag_seq_tensor, word_mask_tensor, tag_mask_tensor,context_seq_tensor, context_mask_tensor = batched_data\n",
    "    if not config['model']['context']:\n",
    "        context_seq_tensor, context_mask_tensor = None, None\n",
    "    _, slot_loss = model.forward(word_seq_tensor,\n",
    "                                                 word_mask_tensor,\n",
    "                                                 tag_seq_tensor,\n",
    "                                                 tag_mask_tensor,\n",
    "                                                context_seq_tensor,\n",
    "                                                context_mask_tensor )\n",
    "\n",
    "    train_slot_loss += slot_loss.item()\n",
    "    loss = slot_loss\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    if config['model']['finetune']:\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "    model.zero_grad()\n",
    "    if step % check_step == 0:\n",
    "        train_slot_loss = train_slot_loss / check_step\n",
    "        print('[%d|%d] step' % (step, max_step))\n",
    "        print('\\t slot loss:', train_slot_loss)\n",
    "        predict_golden = { 'slot': [], 'overall': []}\n",
    "        val_slot_loss= 0\n",
    "        model.eval()\n",
    "        for pad_batch, ori_batch, real_batch_size in dataloader.yield_batches(batch_size, data_key='val'):\n",
    "            pad_batch = tuple(t.to(DEVICE) for t in pad_batch)\n",
    "            word_seq_tensor, tag_seq_tensor, word_mask_tensor, tag_mask_tensor, context_seq_tensor, context_mask_tensor = pad_batch\n",
    "            if not config['model']['context']:\n",
    "                context_seq_tensor, context_mask_tensor = None, None\n",
    "\n",
    "            with torch.no_grad():\n",
    "                slot_logits,  slot_loss = model.forward(word_seq_tensor,  word_mask_tensor,\n",
    "                                                                            tag_seq_tensor,\n",
    "                                                                            tag_mask_tensor,\n",
    "                                                                            context_seq_tensor,\n",
    "                                                                            context_mask_tensor)\n",
    "            val_slot_loss += slot_loss.item() * real_batch_size\n",
    "            for j in range(real_batch_size):\n",
    "                predicts = recover_intent(dataloader, slot_logits[j], tag_mask_tensor[j],\n",
    "                                          ori_batch[j][0], ori_batch[j][1])\n",
    "                labels = ori_batch[j][2]\n",
    "                print('labels',labels)\n",
    "\n",
    "                predict_golden['overall'].append({\n",
    "                    'predict': predicts,\n",
    "                    'golden': labels\n",
    "                })\n",
    "                predict_golden['slot'].append({\n",
    "                    'predict': [x for x in predicts if is_slot_da(x)],\n",
    "                    'golden': [x for x in labels if is_slot_da(x)]\n",
    "                })\n",
    "        for j in range(10):\n",
    "            writer.add_text('val_sample_{}'.format(j),\n",
    "                            json.dumps(predict_golden['overall'][j], indent=2, ensure_ascii=False),\n",
    "                            global_step=step)\n",
    "\n",
    "        total = len(dataloader.data['val'])\n",
    "        val_slot_loss /= total\n",
    "        print('%d samples val' % total)\n",
    "        print('\\t slot loss:', val_slot_loss)\n",
    "\n",
    "        writer.add_scalar('slot_loss/train', train_slot_loss, global_step=step)\n",
    "        writer.add_scalar('slot_loss/val', val_slot_loss, global_step=step)\n",
    "\n",
    "        for x in ['slot', 'overall']:\n",
    "            precision, recall, F1 = calculateF1(predict_golden[x])\n",
    "            print('-' * 20 + x + '-' * 20)\n",
    "            print('\\t Precision: %.2f' % (100 * precision))\n",
    "            print('\\t Recall: %.2f' % (100 * recall))\n",
    "            print('\\t F1: %.2f' % (100 * F1))\n",
    "\n",
    "            writer.add_scalar('val_{}/precision'.format(x), precision, global_step=step)\n",
    "            writer.add_scalar('val_{}/recall'.format(x), recall, global_step=step)\n",
    "            writer.add_scalar('val_{}/F1'.format(x), F1, global_step=step)\n",
    "\n",
    "        if F1 > best_val_f1:\n",
    "            best_val_f1 = F1\n",
    "            torch.save(model.state_dict(), os.path.join(output_dir, 'pytorch_model.bin'))\n",
    "            print('best val F1 %.4f' % best_val_f1)\n",
    "            print('save on', output_dir)\n",
    "\n",
    "        train_slot_loss= 0\n",
    "\n",
    "writer.add_text('val overall F1', '%.2f' % (100 * best_val_f1))\n",
    "writer.close()\n",
    "\n",
    "model_path = os.path.join(output_dir, 'pytorch_model.bin')\n",
    "zip_path = config['zipped_model_path']\n",
    "print('zip model to', zip_path)\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
    "    zf.write(model_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
